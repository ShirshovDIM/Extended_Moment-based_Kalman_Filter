# Расширенный MKF

Участники:
 
[Ширшов Дмитрий (капитан)](https://github.com/ShirshovDIM) (также псевдоним ```MAINPROTOSPACE: SHIRSHOV DMITRY``` в коммитах)

[Кривошей Никита](https://github.com/nkrivoshey)

[Шамсутдинов Аяз](https://github.com/Ayazor)

## Пререквизиты
```
CMake >= 3.22
```

## Запуск
```shell
git clone https://github.com/ShirshovDIM/Extended_MKF.git
cd MKF
mkdir build && cd build
cmake ..
make -j8
```

- Запустить стандартный эксперимент для MKF
```shell
./main
```

## Технический отчет о проделанной работе

Помимо настроечных работ для штатной сборки и запуска приложения были проведены следующие действия, 

непосредственно затрагивающие логику исходного приложения с точки зрения оптимизации: 

- Задизайнен и реализован шаблон для введения новых алгоритмов, аппроксимирующих обратные матрицы (модули ```simple_vehicle_nkf.h```, ```simple_vehicle_nkf.cpp```). Шаблон учитывает типовой IO для того, чтобы оперировать внутри основного исполняемого модуля (в нашем случае endpoint находится в файле ```main.cpp```). 
- Реализованы алгоритмы BFGS, DFP и LBFGS для приближения обратной матрицы. 
- Реализован метод Ньютона-Шульца для приближения обратной матрицы 
- Проведена оптимизация утилизации памяти при формировании Kalman gain (метод ```update``` класса ```SimpleVehicleNKF```)
- Прочие доработки в рамках сборки и запуска проекта 

## Детализация технического отчета 

### Шаблонизация методов аппроксимации и прочие целевые доработки

В рамках доработки проекта реализована расширенная логика, позволяющую выбирать способ вычисления (точнее способ приближения) обратной матрицы, необходимой для расчёта Kalman gain в фильтре Калмана MKF.

Вместо того чтобы всегда использовать стандартный метод S.inverse() (который в рамках нашей реализации доработки называется DIRECT), была добавлена возможность применять n альтернативных аппроксимационных методов, написанных по шаблону в рамках договоренностей. В нашем случае это следующие алгоритмы(http://www.machinelearning.ru/wiki/images/1/10/MOMO18_Seminar6.pdf): 
- BFGS
- DFP
- L‑BFGS
- Newton-Schultz
- DIRECT

Для реализованных в нашем проекте алгоритмов происходит итеративный эксперимент с измерением скорости работы, стандартного отклонения и среднего ошибки при для каждого метода приближения. 

Конечные результаты измерений работы сохраняются в файл ```metrics_nkf.csv```

### Аппроксимирование обратной матрицы по алгоритму BFGS

**Постановка задачи**  

Дана матрица $S \in \mathbb{R}^{n \times n}$. Требуется найти матрицу $J$, такую что:

$$
J \, S = I,
$$

где $I$ — единичная матрица.

Для этого вводят целевую функцию (по Фробениусу):

$$
f(J) = \tfrac{1}{2}\,\|\,J \, S - I\|_F^2
$$

**Итерационный процесс**

1. **Инициализация**

   - Задаём начальное приближение $J_0 = I$
   - Размерность задачи в векторном представлении равна $n^2$. Поэтому для BFGS-приближения Гессиана инициализируем
     $$
       H_0 = I_{\,n^2},
     $$
     где $I_{\,n^2}$ — единичная матрица $n^2 \times n^2$ (векторно-матричное представление).  
   - Градиент целевой функции в матричном виде:
     $$
       \nabla f(J) = \bigl(J S - I \bigr) \, S^T.
     $$
     Если работаем строго с векторами, разворачиваем $J$ и $\nabla f(J)$ в $\mathbb{R}^{n^2}$

2. **Основной шаг итерации (BFGS)**  

   Пусть мы имеем $J_k$ и текущее приближение $H_k$. Тогда:

   1. **Вычислить градиент** при $J_k$:
      $$
        g_k = \nabla f(J_k).
      $$
      Если $\|g_k\| < \text{tol}$, останавливаемся (достигнута требуемая точность).

   2. **Определить направление поиска**:

      $$
        p_k = -\, H_k \, g_k.
      $$

   3. **Одномерный поиск (line search)** :

      Ищем $\alpha_k$, минимизирующий $f(J_k + \alpha \,\text{mat}(p_k))$, где $\text{mat}(p_k)$ — операция обратного свёртывания вектора $p_k$ в матрицу.  

   4. **Обновить $J$**:

      $$
        J_{k+1} \;=\; J_k \;+\; \alpha_k \,\text{mat}(p_k).
      $$

   5. **Вычислить новый градиент**:

      $$
        g_{k+1} = \nabla f(J_{k+1}).
      $$

   6. **Обновить приближение \( H \)** (BFGS-формула):  
   
      $$
        s_k \;=\; \alpha_k \, p_k, 
        \quad 
        y_k \;=\; g_{k+1} \;-\; g_k, 
        \quad 
        \rho_k \;=\; \frac{1}{s_k^T \,y_k} \quad (\text{если } s_k^T \,y_k \neq 0).
      $$
      Тогда  
      $$
        H_{k+1} 
        \;=\; \bigl(I - \rho_k \, s_k \, y_k^T \bigr) \, H_k \, \bigl(I - \rho_k \, y_k \, s_k^T \bigr) 
        \;+\;\rho_k \, s_k \, s_k^T.
      $$

3. **Условие остановки**  
   - Выполняется, если $\|\,g_{k+1}\| < \text{tol}$ (норма нового градиента меньше заданного порога).
   - Или если достигнуто максимальное число итераций $\text{maxIters}$.


### Аппроксимирование обратной матрицы по алгоритму L-BFGS 

**Постановка задачи**  

Дана матрица $S \in \mathbb{R}^{n \times n}$. Требуется найти матрицу $J$, такую что:

$$
J \, S = I,
$$

где $I$ — единичная матрица.

Для этого вводим целевую функцию (по Фробениусу):

$$
f(J) = \tfrac{1}{2}\,\|\,J \, S - I\|_F^2.
$$


**Итерационный процесс**

1. **Инициализация**

   - Задаём начальное приближение $J_0 = I$.
   - Размерность задачи (при векторизации матрицы $J$) равна $n^2$.  
   - Вместо хранения полного BFGS-приближения обратного Гессиана $\bigl(H \in \mathbb{R}^{n^2 \times n^2}\bigr)$, L-BFGS хранит только несколько последних пар $(s_k,\,y_k)$ в числе не более $m$.  

   - Градиент целевой функции в матричном виде:
     $$
       \nabla f(J) \;=\; \bigl(J \, S - I \bigr) \, S^T.
     $$

   - При работе с векторами, $J$ и $\nabla f(J)$ разворачиваются в $\mathbb{R}^{n^2}$.

2. **Основной шаг итерации (L-BFGS)**  

   Пусть мы имеем текущее приближение $J_k$. Определим:
   $$
   g_k = \nabla f(J_k).
   $$

   Если $\|g_k\| < \text{tol}$, то процесс останавливается (достигнута требуемая точность). Иначе:

   1. **Двухшаговая рекурсия (two-loop recursion)** для вычисления приближённого направления $-H_k\,g_k$**:

      - Обозначим $q = g_k$.
      - Идём **в обратном порядке** по сохранённым парам $\{(s_i, y_i)\}$:
        $$
        \alpha_i \;=\; \frac{s_i^T \, q}{\,y_i^T \, s_i\,}, 
        \quad
        q \;\leftarrow\; q \;-\; \alpha_i \, y_i.
        $$
      - Определим
        $$
        \gamma_k \;=\; \frac{s_{\ell}^T \, y_{\ell}}{\,y_{\ell}^T \, y_{\ell}\,},
        $$
        где $s_{\ell}, y_{\ell}$ — последние добавленные векторы (то есть пара из конца списка). Если нет предыдущих пар, можно взять $\gamma_k = 1$.

      - Зададим
        $$
        r \;=\; \gamma_k \; q.
        $$

      - Идём **в прямом порядке**:
        $$
        \beta \;=\; \frac{\,y_i^T \, r\,}{\,y_i^T \, s_i\,},
        \quad
        r \;\leftarrow\; r \;+\; s_i \,(\,\alpha_i - \beta\,).
        $$

      - Вектор $p_k = -r$ будет направлением поиска (аналогично $-H_k \, g_k$ в классическом BFGS).

   2. **Выполнить одномерный поиск** (line search) или взять некоторую эвристику для шага:
      $$
      \alpha_k = \arg\min_\alpha \; f\bigl(J_k + \alpha \,\text{mat}(p_k)\bigr),
      $$
      где $\text{mat}(p_k)$ — разворачивание вектора $p_k$ обратно в матрицу размерности $n \times n$.  

   3. **Обновить $J$**:
      $$
      J_{k+1} \;=\; J_k \;+\; \alpha_k\,\text{mat}(p_k).
      $$

   4. **Вычислить новый градиент**:
      $$
      g_{k+1} \;=\; \nabla f(J_{k+1}).
      $$

   5. **Сохранить новые пары $(s_{k},\,y_{k})$**:

      - Определим
        $$
        s_{k} \;=\; \alpha_k \, p_k,
        \quad
        y_{k} \;=\; g_{k+1} - g_k.
        $$
        (Это в векторном представлении; если $\text{mat}\bigl(s_k\bigr)$ слишком близка к 0, — условие на $\,s_k^T \, y_k$, — то обновление допустимо пропустить)

      - Добавляем $\bigl(s_{k},\, y_{k}\bigr)$ в историю. Если размер истории превышает $m$, удаляем самую старую пару.  

3. **Условие остановки**  
   - Выполняется, если $\|g_{k+1}\| < \text{tol}$.  
   - Или если достигнуто максимальное число итераций $\text{maxIters}$.



### Аппроксимирование обратной матрицы по алгоритму DFP 

**Постановка задачи**  

Дана матрица $S \in \mathbb{R}^{n \times n}$. Требуется найти матрицу $J$, такую что:

$$
J \, S = I,
$$

где $I$ — единичная матрица.  

Для этого используем целевую функцию (например, по норме Фробениуса):

$$
f(J) \;=\; \tfrac12 \,\bigl\|\,J\,S \;-\; I \bigr\|_F^2.
$$


**Итерационный процесс (DFP)**

1. **Инициализация**

   - Задаём начальное приближение $J_0 = I$ (матрица $n \times n$).
   - В векторном виде $J$ имеет размерность $n^2$. Инициализируем приближение Гессиана
     $$
       B_0 = I_{\,n^2},
     $$
     где $I_{\,n^2}$ — единичная матрица $n^2 \times n^2$.

   - Градиент целевой функции в матричном виде:
     $$
       \nabla f(J) = (\,J\,S - I\,)\,S^T.
     $$
     При работе с векторами (разворачивание $J$ и $\nabla f(J)$ в $\mathbb{R}^{n^2}$) всё также.

2. **Основной цикл алгоритма**

   На $k$-м шаге имеем текущие $J_k$ и $B_k$. Тогда:

   1. **Вычислить градиент**:
      $$
        g_k = \nabla f(J_k).
      $$
      Если $\|g_k\| < \text{tol}$, алгоритм останавливается (достигнута требуемая точность).

   2. **Определить направление поиска**:

      Полагаем $H_k = B_k^{-1}$ (обратная к $B_k$ матрица). Тогда
      $$
        p_k \;=\; -\,H_k \; g_k.
      $$
      (В коде это делается путём непосредственного вычисления $B_k^{-1}$.)

   3. **Одномерный поиск (line search)**  
      Ищем $\alpha_k$, минимизирующий
      $$
      f\bigl(J_k + \alpha\, \text{mat}(p_k)\bigr),
      $$
      где $\text{mat}(p_k)$ — «свёртывание» вектора $p_k$ обратно в форму $n \times n$.  
      (В примере кода берётся некоторая эвристика для $\alpha$.)

   4. **Обновить $J$**:
      $$
      J_{k+1} \;=\; J_k \;+\; \alpha_k \,\text{mat}(p_k).
      $$
      В векторном виде это $\;J_{k+1}^{\text{(vec)}} = J_k^{\text{(vec)}} + \alpha_k\,p_k.$

   5. **Новый градиент**:
      $$
      g_{k+1} = \nabla f(J_{k+1}).
      $$

   6. **Обновить матрицу $B$** (DFP-правило):

      Определим
      $$
        s_k \;=\; \alpha_k \, p_k,
        \quad
        y_k \;=\; g_{k+1} - g_k.
      $$

      - Проверим $\,y_k^T \, s_k \neq 0\,$ (иначе обновление может быть неустойчивым).  
      - Вычислим
        $$
        y_k^T \, s_k = \text{скаляр} \quad (\text{обозначим это } y_k^\top s_k).
        $$
      - Аналогично, проверяем $\,s_k^\top B_k \, s_k \neq 0$.  

      Тогда «правило DFP»:
      $$
        B_{k+1}
        \;=\;
        B_k \;+\; \frac{\,y_k\,y_k^T\,}{\,y_k^T\,s_k\,} 
        \;-\;
        \frac{B_k \, s_k \, s_k^T \, B_k}{\,s_k^T\,B_k\,s_k\,}.
      $$

3. **Условие остановки**

   - Выполняется, если $\|g_{k+1}\| < \text{tol}$.
   - Или при достижении максимального числа итераций $\text{maxIters}$.
  



### Алгоритм Ньютона-Шульца (Newton-Schultz)

**Постановка задачи:**

Дана матрица $S \in \mathbb{R}^{n \times n}$. Требуется найти матрицу $X$, такую что:

$
X \cdot S = I,
$

где $I$ — единичная матрица

**Итерационный процесс:**
1. Начальное приближение:
   
   $
   X_0 = \frac{1}{\|S\|} \cdot I,
   $
   
   где $\|S\|$ — норма матрицы $S$ (например, спектральная или Фробениуса)

2. Обновление на $k$-й итерации:
   
   $
   X_{k+1} = X_k \cdot (2I - X_k \cdot S),
   $
   
   что эквивалентно:
   
   $
   X_{k+1} = X_k \cdot (I + R_k), \quad \text{где } R_k = I - X_k \cdot S
   $

3. Условие остановки:
   
   $
   \|R_k\| < \text{tol},
   $
   
   где $\text{tol}$ — заданная точность, $R_k = I - X_k \cdot S$, $\|\cdot\|$ — матричная норма

### Оптимизация потребления памяти для Kalman gain 

Вычисление Kalman Gain
Ключевое вычисление в методе update — это коэффициент усиления Калмана $\mathbf{K}$. В стандартном подходе он вычисляется как:$$\mathbf{K} = \mathbf{\Sigma}_{xz} \cdot \mathbf{S}^{-1}$$
Для матриц $\mathbf{\Sigma}_{xz} \in \mathbb{R}^{3 \times 2}$ и $\mathbf{S} \in \mathbb{R}^{2 \times 2}$:

Вычисление $\mathbf{S}^{-1}$ требует обращения матрицы $2 \times 2$, что в общем случае имеет сложность $O(1)$, но может быть численно нестабильным.
Умножение $\mathbf{\Sigma}_{xz} \cdot \mathbf{S}^{-1}$ требует $3 \times 2 \times 2 = 12$ операций умножения.

Это вычисление может стать затратным для больших матриц или при частых вызовах метода update, особенно если $\mathbf{S}$ плохо обусловлена.

Мы применяем оптимизацию:

Диагональная аппроксимация $\mathbf{S}^{-1}$:Если $\mathbf{S}$ диагональна, то $\mathbf{S}^{-1}$ тоже диагональна, и её элементы вычисляются как:$$\mathbf{S}^{-1}(i,i) = \frac{1}{\mathbf{S}(i,i)}, \quad i = 0, 1$$

Это позволяет избежать полного обращения матрицы $\mathbf{S}$, снижая сложность с $O(1)$ до двух делений для $2 \times 2$.

Умножение с использованием спарс-матрицы:Вместо полного умножения $\mathbf{\Sigma}{xz} \cdot \mathbf{S}^{-1}$ мы итерируемся только по ненулевым элементам $\mathbf{\Sigma}{xz}$, представленной в спарс-формате:$$\mathbf{K}(i,j) = \mathbf{\Sigma}_{xz}(i,j) \cdot \mathbf{S}^{-1}(j,j)$$

Мы выполняем умножение только для ненулевых элементов, что снижает число операций.


Упрощение $\mathbf{S}^{-1}$: Вместо обращения матрицы $2 \times 2$ (что требует $O(1)$ операций, но может быть численно нестабильным), мы выполняем 2 деления.
Снижение числа умножений: В стандартном подходе умножение $\mathbf{\Sigma}{xz} \cdot \mathbf{S}^{-1}$ требует $3 \times 2 \times 2 = 12$ операций умножения. Если в $\mathbf{\Sigma}{xz}$ только 3 ненулевых элемента, мы выполняем только 3 умножения.
Теоретический выигрыш: Для больших матриц эффект был бы значительным. Например, для $\mathbf{S} \in \mathbb{R}^{100 \times 100}$ и $\mathbf{\Sigma}_{xz} \in \mathbb{R}^{200 \times 100}$ с 5% ненулевых элементов:
Обращение $\mathbf{S}$ требует $O(100^3) = O(10^6)$ операций, а диагональная аппроксимация — $O(100)$.
Умножение $\mathbf{\Sigma}_{xz} \cdot \mathbf{S}^{-1}$ требует $200 \times 100 \times 100 = 2 \times 10^6$ операций, а в спарс-формате — $200 \times 100 \times 0.05 = 1000$.


В нашей реализации эффект оптимизации минимален, так как:

Размеры матриц малы: $\mathbf{S} \in \mathbb{R}^{2 \times 2}$, $\mathbf{\Sigma}_{xz} \in \mathbb{R}^{3 \times 2}$. Стандартный метод и так выполняется быстро.

Реализованная оптимизация направлена на ускорение вычисления $\mathbf{K}$ за счёт использования свойств диагональности $\mathbf{S}$ и разреженности $\mathbf{\Sigma}{xz}$. Она снижает число операций умножения и упрощает обращение матрицы $\mathbf{S}$, что особенно эффективно для больших матриц. В текущей реализации эффект минимален из-за малых размеров матриц и плотности $\mathbf{\Sigma}{xz}$, но для больших систем выигрыш в производительности был бы значительным.


### Прочие нецелевые доработки

- доработана логика сборки проекта через CMake (```CMakeList.txt```)
- обновлено логирование основного эксперимента (модуль ```main.cpp```)
- обновлена логика построения графиков - добавлены дополнительные гистограммы для удобства в сравнительном анализе конечной доработки 
- расширена логика генерации синтетических данных
